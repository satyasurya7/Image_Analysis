{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %mkdir checkpoint best_model \n",
    "# creating directories to store checkpoint and best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing Libraries and creating helper functions\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if CUDA is available\n",
    "use_cuda = torch.cuda.is_available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving Function \n",
    "# save_ckp is created to save checkpoint, the latest one and the best one.\n",
    "\n",
    "def save_ckp(state, is_best, checkpoint_path, best_model_path):\n",
    "    '''\n",
    "    state: checkpoint we want to save\n",
    "    is_best: is this the best checkpoint; min validation loss\n",
    "    checkpoint_path: path to save checkpoint\n",
    "    best_model_path: path to save best model\n",
    "\n",
    "    '''\n",
    "    f_path = checkpoint_path\n",
    "    # save checkpoint data to the given path, checkpoint_path\n",
    "    torch.save(state, f_path)\n",
    "    # if it is a best model, min validation loss\n",
    "    if is_best:\n",
    "        best_fpath = best_model_path\n",
    "        # copy that checkpoint file to best path given, best_model_path\n",
    "        shutil.copyfile(f_path, best_fpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "epoch: a measure of the number of times all of the training vectors are used once to update the weights.\n",
    "\n",
    "valid_loss_min: the minimum validation loss, this is needed so that when we continue the training, we can start with this rather than np.Inf value.\n",
    "\n",
    "state_dict: model architecture information. It includes the parameter matrices for each of the layers.\n",
    "\n",
    "optimizer: You meed to save optimizer patameters especially when you are using Adam as your optimizer. Adam is adaptive learning rate method, which means, it conputes individual learning rates for different parameters which we woulld need if we want to continue our training from where we left off.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ckp(checkpoint_fpath, model, optimizer):\n",
    "    \"\"\"\n",
    "    checkpoint_path: path to save checkpoint\n",
    "    model: model that we want to load checkpoint parameters into       \n",
    "    optimizer: optimizer we defined in previous training\n",
    "    \"\"\"\n",
    "    # load check point\n",
    "    checkpoint = torch.load(checkpoint_fpath)\n",
    "    # initialize state_dict from checkpoint to model\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    # initialize optimizer from checkpoint to optimizer\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    # initialize valid_loss_min from checkpoint to valid_loss_min\n",
    "    valid_loss_min = checkpoint['valid_loss_min']\n",
    "    # return model, optimizer, epoch value, min validation loss \n",
    "    return model, optimizer, checkpoint['epoch'], valid_loss_min.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load_chkp is created for loading model. It takes:\n",
    "-location of the saved checkpoint\n",
    "-model instance that you want to load the state to\n",
    "-the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "\n",
    "# Download and load the training data\n",
    "trainset = datasets.FashionMNIST('F_MNIST_data/', download=True, train=True, transform=transform)\n",
    "\n",
    "# Download and load the test data\n",
    "testset = datasets.FashionMNIST('F_MNIST_data/', download=True, train=False, transform=transform)\n",
    "\n",
    "loaders = {\n",
    "    'train' : torch.utils.data.DataLoader(trainset,batch_size = 64,shuffle=True),\n",
    "    'test'  : torch.utils.data.DataLoader(testset,batch_size = 64,shuffle=True),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining and creating a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your network ( Simple Example )\n",
    "class FashionClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        input_size = 784\n",
    "        self.fc1 = nn.Linear(input_size, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 128)\n",
    "        self.fc4 = nn.Linear(128, 64)\n",
    "        self.fc5 = nn.Linear(64,10)\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        x = self.dropout(F.relu(self.fc2(x)))\n",
    "        x = self.dropout(F.relu(self.fc3(x)))\n",
    "        x = self.dropout(F.relu(self.fc4(x)))\n",
    "        x = F.log_softmax(self.fc5(x), dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FashionClassifier(\n",
      "  (fc1): Linear(in_features=784, out_features=512, bias=True)\n",
      "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (fc3): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (fc4): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc5): Linear(in_features=64, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create the network, define the criterion and optimizer\n",
    "model = FashionClassifier()\n",
    "\n",
    "# move model to GPU if CUDA is available\n",
    "if use_cuda:\n",
    "    model = model.cuda()\n",
    "    \n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the network and saving the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train function gives us the ability to set the number of epochs, the learning rate, and other parameters.\n",
    "define loss function and optimizer\n",
    "Below, we are using an Adam optimizer and cross entropy loss since we are looking at character class scores as output. We calculate the loss and perform back-propagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define loss function and optimizer\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define train method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(start_epochs, n_epochs, valid_loss_min_input, loaders, model, optimizer, criterion, use_cuda, checkpoint_path, best_model_path):\n",
    "    \"\"\"\n",
    "    Keyword arguments:\n",
    "    start_epochs -- the real part (default 0.0)\n",
    "    n_epochs -- the imaginary part (default 0.0)\n",
    "    valid_loss_min_input\n",
    "    loaders\n",
    "    model\n",
    "    optimizer\n",
    "    criterion\n",
    "    use_cuda\n",
    "    checkpoint_path\n",
    "    best_model_path\n",
    "    \n",
    "    returns trained model\n",
    "    \"\"\"\n",
    "    # initialize tracker for minimum validation loss\n",
    "    valid_loss_min = valid_loss_min_input \n",
    "    \n",
    "    for epoch in range(start_epochs, n_epochs+1):\n",
    "        # initialize variables to monitor training and validation loss\n",
    "        train_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "        \n",
    "        ###################\n",
    "        # train the model #\n",
    "        ###################\n",
    "        model.train()\n",
    "        for batch_idx, (data, target) in enumerate(loaders['train']):\n",
    "            # move to GPU\n",
    "            if use_cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            ## find the loss and update the model parameters accordingly\n",
    "            # clear the gradients of all optimized variables\n",
    "            optimizer.zero_grad()\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model(data)\n",
    "            # calculate the batch loss\n",
    "            loss = criterion(output, target)\n",
    "            # backward pass: compute gradient of the loss with respect to model parameters\n",
    "            loss.backward()\n",
    "            # perform a single optimization step (parameter update)\n",
    "            optimizer.step()\n",
    "            ## record the average training loss, using something like\n",
    "            ## train_loss = train_loss + ((1 / (batch_idx + 1)) * (loss.data - train_loss))\n",
    "            train_loss = train_loss + ((1 / (batch_idx + 1)) * (loss.data - train_loss))\n",
    "        \n",
    "        ######################    \n",
    "        # validate the model #\n",
    "        ######################\n",
    "        model.eval()\n",
    "        for batch_idx, (data, target) in enumerate(loaders['test']):\n",
    "            # move to GPU\n",
    "            if use_cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            ## update the average validation loss\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model(data)\n",
    "            # calculate the batch loss\n",
    "            loss = criterion(output, target)\n",
    "            # update average validation loss \n",
    "            valid_loss = valid_loss + ((1 / (batch_idx + 1)) * (loss.data - valid_loss))\n",
    "            \n",
    "        # calculate average losses\n",
    "        train_loss = train_loss/len(loaders['train'].dataset)\n",
    "        valid_loss = valid_loss/len(loaders['test'].dataset)\n",
    "\n",
    "        # print training/validation statistics \n",
    "        print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "            epoch, \n",
    "            train_loss,\n",
    "            valid_loss\n",
    "            ))\n",
    "        \n",
    "        # create checkpoint variable and add important data\n",
    "        checkpoint = {\n",
    "            'epoch': epoch + 1,\n",
    "            'valid_loss_min': valid_loss,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "        }\n",
    "        \n",
    "        # save checkpoint\n",
    "        save_ckp(checkpoint, False, checkpoint_path, best_model_path)\n",
    "        \n",
    "        ## TODO: save the model if validation loss has decreased\n",
    "        if valid_loss <= valid_loss_min:\n",
    "            print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,valid_loss))\n",
    "            # save checkpoint as best model\n",
    "            save_ckp(checkpoint, True, checkpoint_path, best_model_path)\n",
    "            valid_loss_min = valid_loss\n",
    "            \n",
    "    # return trained model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.000010 \tValidation Loss: 0.000047\n",
      "Validation loss decreased (inf --> 0.000047).  Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 0.000007 \tValidation Loss: 0.000039\n",
      "Validation loss decreased (0.000047 --> 0.000039).  Saving model ...\n",
      "Epoch: 3 \tTraining Loss: 0.000006 \tValidation Loss: 0.000037\n",
      "Validation loss decreased (0.000039 --> 0.000037).  Saving model ...\n"
     ]
    }
   ],
   "source": [
    "trained_model = train(1, 3, np.Inf, loaders, model, optimizer, criterion, use_cuda, \"./checkpoint/current_checkpoint.pt\", \"./best_model/best_model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a few parameters we used above:\n",
    "\n",
    "start_epoch: value start of the epoch for the training\n",
    "\n",
    "n_epochs: value end of the epoch for the training\n",
    "\n",
    "valid_loss_min_input = np.Inf\n",
    "\n",
    "checkpoint_path: full path to save state of latest checkpoint of the training\n",
    "\n",
    "best_model_path: full path to best state of latest checkpoint of the training\n",
    "\n",
    "Verify if the model are saved\n",
    "List down all files in best_model directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter format not correct - \"best_model\".\n"
     ]
    }
   ],
   "source": [
    "%ls ./best_model/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List down all files in checkpoint directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter format not correct - \"checkpoint\".\n"
     ]
    }
   ],
   "source": [
    "%ls ./checkpoint/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the model\n",
    "\n",
    "Reconstruct the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FashionClassifier(\n",
      "  (fc1): Linear(in_features=784, out_features=512, bias=True)\n",
      "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (fc3): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (fc4): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc5): Linear(in_features=64, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = FashionClassifier()\n",
    "\n",
    "# move model to GPU if CUDA is available\n",
    "if use_cuda:\n",
    "    model = model.cuda()\n",
    "    \n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the optimizer and checkpoint file path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define optimzer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# define checkpoint saved path\n",
    "ckp_path = \"./checkpoint/current_checkpoint.pt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the model using load_ckp function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the saved checkpoint\n",
    "model, optimizer, start_epoch, valid_loss_min = load_ckp(ckp_path, model, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print out the values that we get from load_ckp just to make sure everything is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model =  FashionClassifier(\n",
      "  (fc1): Linear(in_features=784, out_features=512, bias=True)\n",
      "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (fc3): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (fc4): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc5): Linear(in_features=64, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "optimizer =  Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.001\n",
      "    weight_decay: 0\n",
      ")\n",
      "start_epoch =  4\n",
      "valid_loss_min =  3.6626544897444546e-05\n",
      "valid_loss_min = 0.000037\n"
     ]
    }
   ],
   "source": [
    "print(\"model = \", model)\n",
    "print(\"optimizer = \", optimizer)\n",
    "print(\"start_epoch = \", start_epoch)\n",
    "print(\"valid_loss_min = \", valid_loss_min)\n",
    "print(\"valid_loss_min = {:.6f}\".format(valid_loss_min))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we load all the information we need, we can continue training, start_epoch = 4. Previously, we train the model from 1 to 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continue Training and/or Inference\n",
    "Continue training\n",
    "We can continue to train our model using the train function and provide the values of checkpoint we get from the load_ckp function above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 \tTraining Loss: 0.000006 \tValidation Loss: 0.000037\n",
      "Epoch: 5 \tTraining Loss: 0.000006 \tValidation Loss: 0.000036\n",
      "Validation loss decreased (0.000037 --> 0.000036).  Saving model ...\n",
      "Epoch: 6 \tTraining Loss: 0.000005 \tValidation Loss: 0.000034\n",
      "Validation loss decreased (0.000036 --> 0.000034).  Saving model ...\n"
     ]
    }
   ],
   "source": [
    "trained_model = train(start_epoch, 6, valid_loss_min, loaders, model, optimizer, criterion, use_cuda, \"./checkpoint/current_checkpoint.pt\", \"./best_model/best_model.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FashionClassifier(\n",
       "  (fc1): Linear(in_features=784, out_features=512, bias=True)\n",
       "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (fc3): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (fc4): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (fc5): Linear(in_features=64, out_features=10, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on 10000 test images: 87.98%\n"
     ]
    }
   ],
   "source": [
    "test_acc = 0.0\n",
    "for samples, labels in loaders['test']:\n",
    "    with torch.no_grad():\n",
    "        samples, labels = samples.cuda(), labels.cuda()\n",
    "        output = trained_model(samples)\n",
    "        # calculate accuracy\n",
    "        pred = torch.argmax(output, dim=1)\n",
    "        correct = pred.eq(labels)\n",
    "        test_acc += torch.mean(correct.float())\n",
    "print('Accuracy of the network on {} test images: {}%'.format(len(testset), round(test_acc.item()*100.0/len(loaders['test']), 2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 \tTraining Loss: 0.000005 \tValidation Loss: 0.000032\n",
      "Validation loss decreased (0.000037 --> 0.000032).  Saving model ...\n",
      "Epoch: 5 \tTraining Loss: 0.000005 \tValidation Loss: 0.000033\n",
      "Epoch: 6 \tTraining Loss: 0.000005 \tValidation Loss: 0.000034\n",
      "Epoch: 7 \tTraining Loss: 0.000004 \tValidation Loss: 0.000033\n",
      "Epoch: 8 \tTraining Loss: 0.000004 \tValidation Loss: 0.000032\n",
      "Validation loss decreased (0.000032 --> 0.000032).  Saving model ...\n",
      "Epoch: 9 \tTraining Loss: 0.000004 \tValidation Loss: 0.000033\n"
     ]
    }
   ],
   "source": [
    "trained_model = train(start_epoch, 9, valid_loss_min, loaders, model, optimizer, criterion, use_cuda, \"./checkpoint/current_checkpoint.pt\", \"./best_model/best_model.pt\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0a60e4ed926fb2b40cefc20a8a8b05117cb192dddfa7826e0c0b06774f528f37"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
